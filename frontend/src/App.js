import React, { Suspense, useEffect, useRef, useState, useMemo, useCallback } from 'react';
import { Canvas, useFrame } from '@react-three/fiber';
import { useGLTF, useTexture, Loader, Environment, useFBX, useAnimations, OrthographicCamera } from '@react-three/drei';
import { MeshStandardMaterial } from 'three/src/materials/MeshStandardMaterial';

import { LinearEncoding, sRGBEncoding } from 'three/src/constants';
import { LineBasicMaterial, MeshPhysicalMaterial, Vector2 } from 'three';

import createAnimation, { resetAnimationState } from './converter';
import blinkData from './blendDataBlink.json';

import * as THREE from 'three';
import axios from 'axios';
const _ = require('lodash');

// ç¡¬ç¼–ç æœåŠ¡å™¨åœ°å€ - è¿™é‡Œä½¿ç”¨ä½ æœåŠ¡å™¨çš„å®é™…IP
const SERVER_IP = "172.16.10.158";
const host = `http://${SERVER_IP}:5000`;
console.log("åç«¯APIåœ°å€:", host);

// WebSocketé…ç½®
const wsUrl = `ws://${SERVER_IP}:5000/ws`;
let websocket = null;
let clientId = 'client_' + Math.random().toString(36).substr(2, 9);

// å½•éŸ³ç›¸å…³å˜é‡
let mediaRecorder = null;
let audioChunks = [];

// ä¿®æ”¹Avatarç»„ä»¶ä»¥æ”¯æŒæµå¼åŠ¨ç”»å¤„ç†
function Avatar({ avatar_url, speak, setSpeak, text, playing, setPlaying, setResponse, setAnimReady, animationData, setAudioElement, isAudioPlaying, currentSegmentIndex }) {
  let gltf = useGLTF(avatar_url);
  let morphTargetDictionaryBody = null;
  let morphTargetDictionaryLowerTeeth = null;
  const mixerRef = useRef(null);
  
  // å­˜å‚¨å½“å‰æ´»åŠ¨çš„åŠ¨ç”»clips
  const activeClipsRef = useRef([]);
  const segmentClipsRef = useRef(new Map()); // å­˜å‚¨æ¯ä¸ªç‰‡æ®µçš„clips

  const [
    bodyTexture,
    eyesTexture,
    teethTexture,
    bodySpecularTexture,
    bodyRoughnessTexture,
    bodyNormalTexture,
    teethNormalTexture,
    hairTexture,
    tshirtDiffuseTexture,
    tshirtNormalTexture,
    tshirtRoughnessTexture,
    hairAlphaTexture,
    hairNormalTexture,
    hairRoughnessTexture,
    ] = useTexture([
    "/images/body.webp",
    "/images/eyes.webp",
    "/images/teeth_diffuse.webp",
    "/images/body_specular.webp",
    "/images/body_roughness.webp",
    "/images/body_normal.webp",
    "/images/teeth_normal.webp",
    "/images/h_color.webp",
    "/images/tshirt_diffuse.webp",
    "/images/tshirt_normal.webp",
    "/images/tshirt_roughness.webp",
    "/images/h_alpha.webp",
    "/images/h_normal.webp",
    "/images/h_roughness.webp",
  ]);

  _.each([
    bodyTexture,
    eyesTexture,
    teethTexture,
    teethNormalTexture,
    bodySpecularTexture,
    bodyRoughnessTexture,
    bodyNormalTexture,
    tshirtDiffuseTexture,
    tshirtNormalTexture,
    tshirtRoughnessTexture,
    hairAlphaTexture,
    hairNormalTexture,
    hairRoughnessTexture
  ], t => {
    t.encoding = sRGBEncoding;
    t.flipY = false;
  });

  bodyNormalTexture.encoding = LinearEncoding;
  tshirtNormalTexture.encoding = LinearEncoding;
  teethNormalTexture.encoding = LinearEncoding;
  hairNormalTexture.encoding = LinearEncoding;

  gltf.scene.traverse(node => {
    if(node.type === 'Mesh' || node.type === 'LineSegments' || node.type === 'SkinnedMesh') {
      node.castShadow = true;
      node.receiveShadow = true;
      node.frustumCulled = false;

      if (node.name.includes("Body")) {
        node.castShadow = true;
        node.receiveShadow = true;
        node.material = new MeshPhysicalMaterial();
        node.material.map = bodyTexture;
        node.material.roughness = 1.7;
        node.material.roughnessMap = bodyRoughnessTexture;
        node.material.normalMap = bodyNormalTexture;
        node.material.normalScale = new Vector2(0.6, 0.6);
        morphTargetDictionaryBody = node.morphTargetDictionary;
        node.material.envMapIntensity = 0.8;
      }

      if (node.name.includes("Eyes")) {
        node.material = new MeshStandardMaterial();
        node.material.map = eyesTexture;
        node.material.roughness = 0.1;
        node.material.envMapIntensity = 0.5;
      }

      if (node.name.includes("Brows")) {
        node.material = new LineBasicMaterial({color: 0x000000});
        node.material.linewidth = 1;
        node.material.opacity = 0.5;
        node.material.transparent = true;
        node.visible = false;
      }

      if (node.name.includes("Teeth")) {
        node.receiveShadow = true;
        node.castShadow = true;
        node.material = new MeshStandardMaterial();
        node.material.roughness = 0.1;
        node.material.map = teethTexture;
        node.material.normalMap = teethNormalTexture;
        node.material.envMapIntensity = 0.7;
      }

      if (node.name.includes("Hair")) {
        node.material = new MeshStandardMaterial();
        node.material.map = hairTexture;
        node.material.alphaMap = hairAlphaTexture;
        node.material.normalMap = hairNormalTexture;
        node.material.roughnessMap = hairRoughnessTexture;
        node.material.transparent = true;
        node.material.depthWrite = false;
        node.material.side = 2;
        node.material.color.setHex(0x000000);
        node.material.envMapIntensity = 0.3;
      }

      if (node.name.includes("TeethLower")) {
        morphTargetDictionaryLowerTeeth = node.morphTargetDictionary;
      }

      if (node.name.includes("TSHIRT")) {
        node.material = new MeshStandardMaterial();
        node.material.map = tshirtDiffuseTexture;
        node.material.roughnessMap = tshirtRoughnessTexture;
        node.material.normalMap = tshirtNormalTexture;
        node.material.color.setHex(0xffffff);
        node.material.envMapIntensity = 0.5;
      }
    }
  });

  const mixer = useMemo(() => {
    const newMixer = new THREE.AnimationMixer(gltf.scene);
    mixerRef.current = newMixer;
    return newMixer;
  }, []);

  // è®¾ç½®åŸºç¡€åŠ¨ç”»ï¼ˆçœ¨çœ¼å’Œé—²ç½®åŠ¨ç”»ï¼‰
  let idleFbx = useFBX('/idle.fbx');
  let { clips: idleClips } = useAnimations(idleFbx.animations);

  idleClips[0].tracks = _.filter(idleClips[0].tracks, track => {
    return track.name.includes("Head") || track.name.includes("Neck") || track.name.includes("Spine2");
  });

  idleClips[0].tracks = _.map(idleClips[0].tracks, track => {
    if (track.name.includes("Head")) {
      track.name = "head.quaternion";
    }

    if (track.name.includes("Neck")) {
      track.name = "neck.quaternion";
    }

    if (track.name.includes("Spine")) {
      track.name = "spine2.quaternion";
    }

    return track;
  });

  // åˆå§‹åŒ–åŸºç¡€åŠ¨ç”»
  useEffect(() => {
    if (!mixer || !morphTargetDictionaryBody) return;

    let idleClipAction = mixer.clipAction(idleClips[0]);
    idleClipAction.play();

    let blinkClip = createAnimation(blinkData, morphTargetDictionaryBody, 'HG_Body', false);
    if (blinkClip) {
      let blinkAction = mixer.clipAction(blinkClip);
      blinkAction.play();
    }

    setAnimReady(true);
  }, [mixer, morphTargetDictionaryBody]);

  // å¤„ç†æ–°çš„åŠ¨ç”»æ•°æ® - å…³é”®ä¿®æ”¹ç‚¹
  useEffect(() => {
    if (!animationData || !animationData.blendData || !morphTargetDictionaryBody || !morphTargetDictionaryLowerTeeth) {
      return;
    }

    console.log("å¤„ç†æ–°åŠ¨ç”»æ•°æ®ç‰‡æ®µï¼Œå¸§æ•°:", animationData.blendData.length);
    console.log("åŠ¨ç”»æ•°æ®ç¤ºä¾‹:", animationData.blendData.slice(0, 2));

    try {
      // åœæ­¢ä¹‹å‰çš„è¡¨æƒ…åŠ¨ç”»ï¼ˆä¿ç•™åŸºç¡€åŠ¨ç”»ï¼‰
      activeClipsRef.current.forEach(clip => {
        const action = mixer.existingAction(clip);
        if (action) {
          action.stop();
          action.reset();
        }
      });

      // åˆ›å»ºæ–°çš„åŠ¨ç”»å‰ªè¾‘
      const bodyClip = createAnimation(animationData.blendData, morphTargetDictionaryBody, 'HG_Body', true);
      const teethClip = createAnimation(animationData.blendData, morphTargetDictionaryLowerTeeth, 'HG_TeethLower', true);

      const newClips = [bodyClip, teethClip].filter(clip => clip !== null);

      if (newClips.length > 0) {
        console.log("åŠ¨ç”»å‰ªè¾‘å·²åˆ›å»º:", newClips.map(c => `${c.tracks.length}ä¸ªè½¨é“`));
        
        // å­˜å‚¨æ–°çš„æ´»åŠ¨clips
        activeClipsRef.current = newClips;

        // æ’­æ”¾æ–°çš„åŠ¨ç”»
        newClips.forEach(clip => {
          if (clip && clip.tracks && clip.tracks.length > 0) {
            const clipAction = mixer.clipAction(clip);
            clipAction.setLoop(THREE.LoopOnce);
            clipAction.clampWhenFinished = true;
            clipAction.reset();
            clipAction.play();
            
            console.log(`æ’­æ”¾åŠ¨ç”»å‰ªè¾‘: ${clip.tracks.length}ä¸ªè½¨é“, æŒç»­æ—¶é—´: ${clip.duration.toFixed(2)}ç§’`);
          }
        });
      } else {
        console.warn("æ— æ³•åˆ›å»ºåŠ¨ç”»å‰ªè¾‘");
      }
    } catch (error) {
      console.error("å¤„ç†åŠ¨ç”»æ•°æ®æ—¶å‡ºé”™:", error);
    }
  }, [animationData, mixer, morphTargetDictionaryBody, morphTargetDictionaryLowerTeeth]);

  // é‡ç½®åŠ¨ç”»çŠ¶æ€
  const resetAllAnimations = useCallback(() => {
    console.log("é‡ç½®æ‰€æœ‰åŠ¨ç”»");
    
    // åœæ­¢æ‰€æœ‰è¡¨æƒ…åŠ¨ç”»
    activeClipsRef.current.forEach(clip => {
      const action = mixer.existingAction(clip);
      if (action) {
        action.stop();
        action.reset();
      }
    });

    // æ¸…ç©ºæ´»åŠ¨åŠ¨ç”»åˆ—è¡¨
    activeClipsRef.current = [];
    segmentClipsRef.current.clear();

    // é‡ç½®åŠ¨ç”»çŠ¶æ€
    resetAnimationState();

    // ç¡®ä¿åŸºç¡€åŠ¨ç”»ç»§ç»­æ’­æ”¾
    if (idleClips[0]) {
      let idleClipAction = mixer.clipAction(idleClips[0]);
      idleClipAction.play();
    }

    if (morphTargetDictionaryBody) {
      let blinkClip = createAnimation(blinkData, morphTargetDictionaryBody, 'HG_Body', false);
      if (blinkClip) {
        let blinkAction = mixer.clipAction(blinkClip);
        blinkAction.play();
      }
    }
  }, [mixer, idleClips, morphTargetDictionaryBody]);

  // ç›‘å¬æ’­æ”¾çŠ¶æ€å˜åŒ–
  useEffect(() => {
    if (!isAudioPlaying && activeClipsRef.current.length > 0) {
      console.log("éŸ³é¢‘æ’­æ”¾ç»“æŸï¼Œé‡ç½®æ‰€æœ‰åŠ¨ç”»");
      resetAllAnimations();
    }
  }, [isAudioPlaying, resetAllAnimations]);

  useFrame((state, delta) => {
    if (mixer) {
      mixer.update(delta);
    }
  });

  return (
    <group name="avatar">
      <primitive object={gltf.scene} dispose={null} />
    </group>
  );
}

// WebSocketè¿æ¥ä¼˜åŒ–
function setupWebSocket(setBackendStatus, setWsReady) {
  if (websocket && websocket.readyState === WebSocket.OPEN) {
    console.log("WebSocketå·²è¿æ¥ï¼Œä¸éœ€è¦é‡æ–°è¿æ¥");
    return websocket;
  }

  // å®Œæ•´çš„WebSocket URL
  const fullWsUrl = `${wsUrl}/${clientId}`;
  console.log("å°è¯•è¿æ¥WebSocket:", fullWsUrl);

  try {
    const ws = new WebSocket(fullWsUrl);

    ws.onopen = () => {
      console.log("WebSocketè¿æ¥æˆåŠŸ");
      setBackendStatus("å·²è¿æ¥ (WebSocket)");
      websocket = ws;
      if (setWsReady) setWsReady(true);

      // å‘é€pingä¿æŒè¿æ¥
      const pingInterval = setInterval(() => {
        if (ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify({ type: "ping" }));
        } else {
          clearInterval(pingInterval);
        }
      }, 30000);
    };

    ws.onerror = (error) => {
      console.error("WebSocketé”™è¯¯:", error);
      setBackendStatus("WebSocketè¿æ¥å¤±è´¥ - ä½¿ç”¨HTTP API");
      websocket = null;
      if (setWsReady) setWsReady(false);
    };

    ws.onclose = () => {
      console.log("WebSocketè¿æ¥å·²å…³é—­");
      setBackendStatus("ä½¿ç”¨HTTP API");
      websocket = null;
      if (setWsReady) setWsReady(false);

      // å°è¯•é‡æ–°è¿æ¥
      setTimeout(() => {
        setupWebSocket(setBackendStatus, setWsReady);
      }, 5000);
    };

    return ws;
  } catch (error) {
    console.error("åˆ›å»ºWebSocketæ—¶å‡ºé”™:", error);
    setBackendStatus("WebSocketå¤±è´¥ - ä½¿ç”¨HTTP API");
    if (setWsReady) setWsReady(false);
    return null;
  }
}

// éº¦å…‹é£åˆå§‹åŒ–
async function setupMicrophone(setMicStatus) {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
        sampleRate: 16000,
        channelCount: 1
      }
    });
    setMicStatus("éº¦å…‹é£å·²å‡†å¤‡å°±ç»ª");
    return stream;
  } catch (error) {
    console.error("è·å–éº¦å…‹é£æƒé™å¤±è´¥:", error);
    setMicStatus(`éº¦å…‹é£é”™è¯¯: ${error.message}`);
    return null;
  }
}

// å½•éŸ³é…ç½®
function startRecording(stream, setIsRecording, setRecordingStatus) {
  if (!stream) return;

  audioChunks = [];

  // é…ç½®MediaRecorder
  const options = {
    mimeType: 'audio/webm',
    audioBitsPerSecond: 16000
  };

  try {
    mediaRecorder = new MediaRecorder(stream, options);
  } catch (e) {
    console.warn("WebMæ ¼å¼ä¸æ”¯æŒï¼Œå°è¯•ä½¿ç”¨æ›¿ä»£æ ¼å¼");
    try {
      mediaRecorder = new MediaRecorder(stream);
    } catch (e2) {
      console.error("æ— æ³•åˆ›å»ºMediaRecorder:", e2);
      setRecordingStatus("æµè§ˆå™¨ä¸æ”¯æŒå½•éŸ³åŠŸèƒ½");
      return;
    }
  }

  mediaRecorder.ondataavailable = (event) => {
    if (event.data.size > 0) {
      audioChunks.push(event.data);
    }
  };

  mediaRecorder.onstart = () => {
    console.log("å½•éŸ³å¼€å§‹");
    setIsRecording(true);
    setRecordingStatus("æ­£åœ¨å½•éŸ³...");
  };

  mediaRecorder.onstop = () => {
    console.log("å½•éŸ³ç»“æŸ");
    setIsRecording(false);
    setRecordingStatus("å½•éŸ³å·²åœæ­¢");
  };

  // æ¯100msä¿å­˜ä¸€æ¬¡æ•°æ®ï¼Œæé«˜å“åº”é€Ÿåº¦
  mediaRecorder.start(100);

  // è®¾ç½®è‡ªåŠ¨åœæ­¢å½•éŸ³çš„è®¡æ—¶å™¨ (8ç§’)
  setTimeout(() => {
    if (mediaRecorder && mediaRecorder.state === "recording") {
      mediaRecorder.stop();
    }
  }, 8000);
}

const STYLES = {
  container: {
    display: 'flex',
    flexDirection: 'column',
    height: '100vh',
    width: '100vw',
    overflow: 'hidden',
    position: 'relative'
  },
  speechArea: {
    position: 'absolute',
    bottom: '20px',
    left: '50%',
    transform: 'translateX(-50%)',
    zIndex: 500,
    display: 'flex',
    flexDirection: 'column',
    alignItems: 'center',
    width: '90%',
    maxWidth: '600px'
  },
  conversationBox: {
    position: 'absolute',
    left: '50%',
    transform: 'translateX(-50%)',
    bottom: '180px',
    width: '90%',
    maxWidth: '600px',
    backgroundColor: 'rgba(0, 0, 0, 0.6)',
    borderRadius: '10px',
    padding: '15px',
    zIndex: 499,
    display: 'flex',
    flexDirection: 'column',
    gap: '10px'
  },
  userBubble: {
    backgroundColor: 'rgba(0, 101, 255, 0.7)',
    padding: '10px 15px',
    borderRadius: '18px 18px 18px 0',
    alignSelf: 'flex-start',
    maxWidth: '80%',
    wordBreak: 'break-word',
    color: 'white',
    marginBottom: '5px'
  },
  aiBubble: {
    backgroundColor: 'rgba(70, 70, 70, 0.7)',
    padding: '10px 15px',
    borderRadius: '18px 18px 0 18px',
    alignSelf: 'flex-end',
    maxWidth: '80%',
    wordBreak: 'break-word',
    color: 'white',
    marginBottom: '5px'
  },
  recordButton: {
    width: '80px',
    height: '80px',
    borderRadius: '50%',
    backgroundColor: '#cc0000',
    border: 'none',
    cursor: 'pointer',
    display: 'flex',
    justifyContent: 'center',
    alignItems: 'center',
    boxShadow: '0 4px 8px rgba(0, 0, 0, 0.3)',
    transition: 'all 0.2s ease',
    marginTop: '15px'
  },
  recordingButton: {
    backgroundColor: '#ff0000',
    boxShadow: '0 0 0 10px rgba(255, 0, 0, 0.3)'
  },
  micIcon: {
    fontSize: '30px',
    color: 'white'
  },
  statusText: {
    color: 'white',
    marginTop: '10px',
    textAlign: 'center',
    fontSize: '16px',
    textShadow: '1px 1px 3px rgba(0, 0, 0, 0.7)'
  },
  statusBar: {
    position: 'absolute',
    bottom: '5px',
    left: '10px',
    color: '#00FF00',
    fontSize: '12px',
    zIndex: 500
  },
  hidden: {
    display: 'none'
  }
};

function App() {
  // çŠ¶æ€å˜é‡
  const [speak, setSpeak] = useState(false);
  const [response, setResponse] = useState("");
  const [playing, setPlaying] = useState(false);
  const [backendStatus, setBackendStatus] = useState("æ­£åœ¨è¿æ¥...");
  const [animReady, setAnimReady] = useState(false);
  const [animationData, setAnimationData] = useState(null);
  const [micStream, setMicStream] = useState(null);
  const [micStatus, setMicStatus] = useState("æ­£åœ¨åˆå§‹åŒ–éº¦å…‹é£...");
  const [isRecording, setIsRecording] = useState(false);
  const [recordingStatus, setRecordingStatus] = useState("ç‚¹å‡»éº¦å…‹é£æŒ‰é’®å¼€å§‹å½•éŸ³");
  const [statusMessage, setStatusMessage] = useState("");
  const [conversation, setConversation] = useState([]);
  const [wsReady, setWsReady] = useState(false);
  
  // æµå¼éŸ³é¢‘å¤„ç†çŠ¶æ€
  const [audioQueue, setAudioQueue] = useState([]);
  const [isAudioPlaying, setIsAudioPlaying] = useState(false);
  const [currentAudioIndex, setCurrentAudioIndex] = useState(0);
  const currentAudioRef = useRef(null);
  const processingRef = useRef(false);
  const responseTextRef = useRef("");

  // é™åˆ¶å¯¹è¯å†å²é•¿åº¦
  useEffect(() => {
    if (conversation.length > 10) {
      setConversation(prev => prev.slice(prev.length - 10));
    }
  }, [conversation]);

  // åœæ­¢å½•éŸ³
  const stopRecording = useCallback(async () => {
    if (!mediaRecorder) return;

    return new Promise((resolve) => {
      mediaRecorder.onstop = async () => {
        console.log("å½•éŸ³å·²å®Œæˆï¼Œå¤„ç†ä¸­...");
        setIsRecording(false);
        setRecordingStatus("å¤„ç†å½•éŸ³...");
        setStatusMessage("æ­£åœ¨å¤„ç†è¯­éŸ³...");

        const audioBlob = new Blob(audioChunks, { type: mediaRecorder.mimeType || 'audio/webm' });
        console.log("å½•éŸ³æ–‡ä»¶å¤§å°:", audioBlob.size, "å­—èŠ‚", "ç±»å‹:", audioBlob.type);

        try {
          // å‘é€éŸ³é¢‘åˆ°æœåŠ¡å™¨
          if (websocket && websocket.readyState === WebSocket.OPEN) {
            // é€šè¿‡WebSocketå‘é€
            sendAudioViaWebSocket(audioBlob);
          } else {
            // é€šè¿‡HTTPè¯·æ±‚å‘é€
            await sendAudioViaHttp(audioBlob);
          }
        } catch (error) {
          console.error("å¤„ç†éŸ³é¢‘æ—¶å‡ºé”™:", error);
          setStatusMessage(`å¤„ç†éŸ³é¢‘å¤±è´¥: ${error.message}`);
        }

        resolve();
      };

      mediaRecorder.stop();
    });
  }, []);

  // é€šè¿‡WebSocketå‘é€éŸ³é¢‘
  const sendAudioViaWebSocket = useCallback((audioBlob) => {
    console.log("é€šè¿‡WebSocketå‘é€éŸ³é¢‘");

    // é‡ç½®æ‰€æœ‰æµå¼å¤„ç†çŠ¶æ€
    setAudioQueue([]);
    setCurrentAudioIndex(0);
    processingRef.current = false;
    responseTextRef.current = "";
    setAnimationData(null);
    
    if (currentAudioRef.current) {
      currentAudioRef.current.pause();
      currentAudioRef.current = null;
    }

    // åˆ›å»ºè¡¨å•æ•°æ®
    const reader = new FileReader();
    reader.onload = () => {
      const base64data = reader.result.split(',')[1];

      websocket.send(JSON.stringify({
        type: "speech_recognition",
        audio_data: base64data,
        audio_format: audioBlob.type || "audio/webm",
        client_id: clientId
      }));

      setStatusMessage("è¯­éŸ³è¯†åˆ«ä¸­...");
    };

    reader.readAsDataURL(audioBlob);
  }, []);

  // é€šè¿‡HTTPå‘é€éŸ³é¢‘
  const sendAudioViaHttp = useCallback(async (audioBlob) => {
    console.log("é€šè¿‡HTTP APIå‘é€éŸ³é¢‘");

    // é‡ç½®æ‰€æœ‰æµå¼å¤„ç†çŠ¶æ€
    setAudioQueue([]);
    setCurrentAudioIndex(0);
    processingRef.current = false;
    responseTextRef.current = "";
    setAnimationData(null);
    
    if (currentAudioRef.current) {
      currentAudioRef.current.pause();
      currentAudioRef.current = null;
    }

    const formData = new FormData();
    formData.append('audio', audioBlob);
    formData.append('client_id', clientId);
    formData.append('audio_format', audioBlob.type || "audio/webm");

    try {
      setStatusMessage("å‘é€éŸ³é¢‘åˆ°æœåŠ¡å™¨...");
      const response = await axios.post(`${host}/recognize_speech`, formData, {
        headers: {
          'Content-Type': 'multipart/form-data'
        }
      });

      console.log("è¯­éŸ³è¯†åˆ«å“åº”:", response.data);
      if (response.data && response.data.text) {
        // æ›´æ–°å¯¹è¯å†å²
        setConversation(prev => [...prev, {
          role: 'user',
          content: response.data.text
        }]);
        setStatusMessage("å¼€å§‹å¤„ç†è¯†åˆ«åˆ°çš„æ–‡æœ¬");
      } else {
        setStatusMessage("è¯­éŸ³è¯†åˆ«å¤±è´¥");
      }
    } catch (error) {
      console.error("å‘é€éŸ³é¢‘å¤±è´¥:", error);
      setStatusMessage(`å‘é€éŸ³é¢‘å¤±è´¥: ${error.message}`);
    }
  }, []);

  // å–æ¶ˆè¿›è¡Œä¸­çš„ä»»åŠ¡
  const cancelProcessingTask = useCallback(() => {
    // åœæ­¢å½“å‰æ’­æ”¾çš„éŸ³é¢‘
    if (currentAudioRef.current) {
      currentAudioRef.current.pause();
      currentAudioRef.current = null;
    }
    
    // é‡ç½®æµå¼å¤„ç†çŠ¶æ€
    setAudioQueue([]);
    setCurrentAudioIndex(0);
    setIsAudioPlaying(false);
    processingRef.current = false;
    responseTextRef.current = "";
    
    // é‡ç½®çŠ¶æ€
    setPlaying(false);
    setAnimationData(null);
    setStatusMessage("");
    
    // é‡ç½®åŠ¨ç”»çŠ¶æ€
    resetAnimationState();
  }, []);

  // å¤„ç†æµå¼éŸ³é¢‘é˜Ÿåˆ—
  useEffect(() => {
    // å½“æœ‰æ–°çš„éŸ³é¢‘æ·»åŠ åˆ°é˜Ÿåˆ—æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å¼€å§‹æ’­æ”¾
    if (audioQueue.length > 0 && !isAudioPlaying && currentAudioIndex < audioQueue.length) {
      console.log(`å¼€å§‹æ’­æ”¾éŸ³é¢‘ç‰‡æ®µ ${currentAudioIndex+1}/${audioQueue.length}`);
      
      const currentSegment = audioQueue[currentAudioIndex];
      const audio = new Audio(currentSegment.audioUrl);
      currentAudioRef.current = audio;
      
      // è®¾ç½®æ’­æ”¾äº‹ä»¶
      audio.oncanplay = () => {
        console.log(`éŸ³é¢‘ç‰‡æ®µ ${currentAudioIndex+1} å·²åŠ è½½ï¼Œå‡†å¤‡æ’­æ”¾`);
        
        // å…³é”®ä¿®æ”¹ï¼šå…ˆè®¾ç½®åŠ¨ç”»æ•°æ®ï¼Œå†æ’­æ”¾éŸ³é¢‘
        if (currentSegment.blendData && currentSegment.blendData.length > 0) {
          console.log(`è®¾ç½®åŠ¨ç”»æ•°æ®ï¼Œå¸§æ•°: ${currentSegment.blendData.length}`);
          setAnimationData({ blendData: currentSegment.blendData });
          setPlaying(true);
          
          // ç¨å¾®å»¶è¿Ÿæ’­æ”¾éŸ³é¢‘ï¼Œç¡®ä¿åŠ¨ç”»å·²è®¾ç½®
          setTimeout(() => {
            audio.play()
              .then(() => {
                console.log(`å¼€å§‹æ’­æ”¾éŸ³é¢‘ç‰‡æ®µ ${currentAudioIndex+1}`);
                setIsAudioPlaying(true);
              })
              .catch(error => {
                console.error(`æ’­æ”¾éŸ³é¢‘ç‰‡æ®µ ${currentAudioIndex+1} å¤±è´¥:`, error);
                playNextAudio();
              });
          }, 50);
        } else {
          console.warn(`éŸ³é¢‘ç‰‡æ®µ ${currentAudioIndex+1} æ²¡æœ‰åŠ¨ç”»æ•°æ®`);
          // å³ä½¿æ²¡æœ‰åŠ¨ç”»æ•°æ®ä¹Ÿæ’­æ”¾éŸ³é¢‘
          audio.play()
            .then(() => {
              console.log(`å¼€å§‹æ’­æ”¾éŸ³é¢‘ç‰‡æ®µ ${currentAudioIndex+1} (æ— åŠ¨ç”»)`);
              setIsAudioPlaying(true);
            })
            .catch(error => {
              console.error(`æ’­æ”¾éŸ³é¢‘ç‰‡æ®µ ${currentAudioIndex+1} å¤±è´¥:`, error);
              playNextAudio();
            });
        }
      };
      
      // å½“å‰éŸ³é¢‘æ’­æ”¾ç»“æŸæ—¶
      audio.onended = () => {
        console.log(`éŸ³é¢‘ç‰‡æ®µ ${currentAudioIndex+1} æ’­æ”¾ç»“æŸ`);
        playNextAudio();
      };
      
      // éŸ³é¢‘åŠ è½½é”™è¯¯
      audio.onerror = () => {
        console.error(`éŸ³é¢‘ç‰‡æ®µ ${currentAudioIndex+1} åŠ è½½å‡ºé”™`);
        playNextAudio();
      };
    }
  }, [audioQueue, isAudioPlaying, currentAudioIndex]);
  
  // æ’­æ”¾ä¸‹ä¸€ä¸ªéŸ³é¢‘ç‰‡æ®µ
  const playNextAudio = useCallback(() => {
    // æ¸…ç†å½“å‰éŸ³é¢‘
    if (currentAudioRef.current) {
      currentAudioRef.current.pause();
      currentAudioRef.current = null;
    }
    
    // æ›´æ–°ç´¢å¼•
    setCurrentAudioIndex(prev => prev + 1);
    setIsAudioPlaying(false);

    // æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç‰‡æ®µéƒ½å·²æ’­æ”¾å®Œæ¯•
    if (currentAudioIndex + 1 >= audioQueue.length) {
      console.log("æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µæ’­æ”¾å®Œæ¯•");
     
      // æ‰€æœ‰ç‰‡æ®µæ’­æ”¾å®Œæ¯•
      if (!processingRef.current) {
        // æ²¡æœ‰æ›´å¤šå¾…å¤„ç†çš„ç‰‡æ®µï¼Œå®Œå…¨ç»“æŸ
        setPlaying(false);
        setStatusMessage("");
      } else {
        // ä»åœ¨å¤„ç†ä¸­ï¼Œç­‰å¾…ä¸‹ä¸€ä¸ªç‰‡æ®µ
        setStatusMessage("ç»§ç»­å¤„ç†æ›´å¤šå†…å®¹...");
      }
    }
  }, [currentAudioIndex, audioQueue.length]);

 // åˆå§‹åŒ–WebSocket
 useEffect(() => {
   const ws = setupWebSocket(setBackendStatus, setWsReady);
 }, []);

 // åˆå§‹åŒ–éº¦å…‹é£
 useEffect(() => {
   async function initMic() {
     const stream = await setupMicrophone(setMicStatus);
     setMicStream(stream);
   }

   initMic();

   // ç»„ä»¶å¸è½½æ—¶å…³é—­éº¦å…‹é£
   return () => {
     if (micStream) {
       micStream.getTracks().forEach(track => track.stop());
     }
   };
 }, []);

 // è®¾ç½®WebSocketæ¶ˆæ¯å¤„ç† - å…³é”®ä¿®æ”¹ç‚¹
 useEffect(() => {
   if (websocket) {
     websocket.onmessage = (event) => {
       try {
         const message = JSON.parse(event.data);
         
         console.log("æ”¶åˆ°WebSocketæ¶ˆæ¯ç±»å‹:", message.type);
         
         // å¤„ç†æµå¼éŸ³é¢‘ç‰‡æ®µ
         if (message.type === "stream_audio_segment") {
           console.log("æ”¶åˆ°æµå¼éŸ³é¢‘ç‰‡æ®µ", {
             segment_index: message.segment_index,
             is_first_segment: message.is_first_segment,
             blendData_length: message.blendData ? message.blendData.length : 0,
             text: message.text
           });
           
           // ç¡®ä¿éŸ³é¢‘URLæ˜¯å®Œæ•´è·¯å¾„
           let audioPath = message.filename;
           if (!audioPath.startsWith('http')) {
             audioPath = `${host}${audioPath}`;
           }
           
           // å°†éŸ³é¢‘ç‰‡æ®µæ·»åŠ åˆ°é˜Ÿåˆ—
           setAudioQueue(prev => [...prev, {
             audioUrl: audioPath,
             blendData: message.blendData,
             text: message.text,
             segmentIndex: message.segment_index
           }]);
           
           // æ›´æ–°å¤„ç†çŠ¶æ€
           processingRef.current = true;
           
           // æ›´æ–°æ–‡æœ¬æ˜¾ç¤º
           if (message.text) {
             responseTextRef.current += message.text;
             setResponse(responseTextRef.current);
             
             // å¦‚æœè¿™æ˜¯ç¬¬ä¸€ä¸ªç‰‡æ®µï¼Œæ·»åŠ åˆ°å¯¹è¯å†å²
             if (message.is_first_segment) {
               setConversation(prev => [...prev, {
                 role: 'assistant',
                 content: responseTextRef.current
               }]);
             } else {
               // æ›´æ–°æœ€åä¸€ä¸ªå¯¹è¯
               setConversation(prev => {
                 const newConversation = [...prev];
                 if (newConversation.length > 0) {
                   newConversation[newConversation.length - 1] = {
                     ...newConversation[newConversation.length - 1],
                     content: responseTextRef.current
                   };
                 }
                 return newConversation;
               });
             }
           }
           
           // æ›´æ–°çŠ¶æ€æ¶ˆæ¯
           setStatusMessage(`å¤„ç†ç‰‡æ®µ ${message.segment_index + 1}...`);
         }
         
         // å¤„ç†å®Œæˆæ‰€æœ‰ç‰‡æ®µ
         else if (message.type === "processing_complete_all") {
           console.log("æ‰€æœ‰ç‰‡æ®µå¤„ç†å®Œæˆ");
           processingRef.current = false;
           
           // æ›´æ–°æœ€ç»ˆå“åº”æ–‡æœ¬
           if (message.message) {
             responseTextRef.current = message.message;
             setResponse(message.message);
             
             // æ›´æ–°å¯¹è¯å†å²ä¸­çš„æœ€åä¸€æ¡æ¶ˆæ¯
             setConversation(prev => {
               const newConversation = [...prev];
               if (newConversation.length > 0) {
                 newConversation[newConversation.length - 1] = {
                   ...newConversation[newConversation.length - 1],
                   content: message.message
                 };
               }
               return newConversation;
             });
           }
           
           setStatusMessage("");
         }
         
         // ä¿ç•™å¯¹æ—§æ¶ˆæ¯ç±»å‹çš„å¤„ç†ä»¥å…¼å®¹æ€§
         else if (message.type === "processing_complete") {
           console.log("å¤„ç†å®Œæˆï¼Œè·å–Blendshapeæ•°æ®å’ŒéŸ³é¢‘");

           // ç¡®ä¿éŸ³é¢‘URLæ˜¯å®Œæ•´è·¯å¾„
           let audioPath = message.filename;
           if (!audioPath.startsWith('http')) {
             audioPath = `${host}${audioPath}`;
           }
           console.log("å®Œæ•´éŸ³é¢‘URL:", audioPath);

           // ä¿å­˜åŠ¨ç”»æ•°æ®
           if (message.blendData && message.blendData.length > 0) {
             console.log("è®¾ç½®åŠ¨ç”»æ•°æ®ï¼Œå¸§æ•°:", message.blendData.length);
             setAnimationData({ blendData: message.blendData });
           }

           // åˆ›å»ºéŸ³é¢‘å…ƒç´ 
           const audio = new Audio(audioPath);
           audio.oncanplay = () => {
             console.log("éŸ³é¢‘å·²åŠ è½½ï¼Œå‡†å¤‡æ’­æ”¾");
             audio.play()
               .then(() => {
                 console.log("å¼€å§‹æ’­æ”¾éŸ³é¢‘");
                 setPlaying(true);
                 setIsAudioPlaying(true);
               })
               .catch(error => {
                 console.error("æ’­æ”¾éŸ³é¢‘å¤±è´¥:", error);
               });
           };

           audio.onended = () => {
             console.log("éŸ³é¢‘æ’­æ”¾ç»“æŸ");
             setPlaying(false);
             setIsAudioPlaying(false);
             setAnimationData(null);
           };

           currentAudioRef.current = audio;

           if (message.text) {
             setResponse(message.text);
             // æ·»åŠ åˆ°å¯¹è¯å†å²
             setConversation(prev => [...prev, {
               role: 'assistant',
               content: message.text
             }]);
           }

           // é‡ç½®çŠ¶æ€æ¶ˆæ¯
           setStatusMessage("");
         }
         else if (message.type === "ai_response") {
           console.log("æ”¶åˆ°AIå“åº”:", message.text);
           if (message.text) {
             setStatusMessage("AIå·²å“åº”ï¼Œæ­£åœ¨ç”Ÿæˆè¯­éŸ³...");
           }
         }
         else if (message.type === "speech_recognition_result") {
           console.log("æ”¶åˆ°è¯­éŸ³è¯†åˆ«ç»“æœ:", message.text);
           if (message.text) {
             // æ·»åŠ åˆ°å¯¹è¯å†å²
             setConversation(prev => [...prev, {
               role: 'user',
               content: message.text
             }]);
             setStatusMessage("è¯†åˆ«å®Œæˆï¼Œç­‰å¾…å›å¤...");
             
             // é‡ç½®æµå¼å¤„ç†çŠ¶æ€
             responseTextRef.current = "";
             setAudioQueue([]);
             setCurrentAudioIndex(0);
             processingRef.current = true;
           } else {
             setStatusMessage("è¯­éŸ³è¯†åˆ«ç»“æœä¸ºç©º");
           }
         }
         else if (message.type === "processing_status") {
           // å¤„ç†çŠ¶æ€æ›´æ–°
           setStatusMessage(message.message || `æ­£åœ¨${message.status}...`);
         }
         else if (message.type === "error") {
           console.error("WebSocketé”™è¯¯:", message.message);
           alert(`å¤„ç†æ—¶å‡ºé”™: ${message.message}`);
           setStatusMessage("");
           
           // é‡ç½®å¤„ç†çŠ¶æ€
           processingRef.current = false;
         }
       } catch (error) {
         console.error("è§£æWebSocketæ¶ˆæ¯æ—¶å‡ºé”™:", error);
         setStatusMessage("");
       }
     };
   }
 }, [websocket]);

 // æ£€æŸ¥åç«¯æœåŠ¡çŠ¶æ€
 useEffect(() => {
   fetch(`${host}/docs`)
     .then(response => {
       if(response.ok) {
         setBackendStatus("REST APIå·²è¿æ¥");
         // å¦‚æœREST APIå¯ç”¨ï¼Œå°è¯•è¿æ¥WebSocket
         setupWebSocket(setBackendStatus, setWsReady);
       } else {
         setBackendStatus("åç«¯æœåŠ¡è¿æ¥å¤±è´¥");
       }
     })
     .catch(err => {
       console.error("åç«¯æœåŠ¡è¿æ¥é”™è¯¯:", err);
       setBackendStatus(`åç«¯æœåŠ¡ä¸å¯ç”¨: ${err.message}`);
     });
 }, []);

 // å¤„ç†å½•éŸ³æŒ‰é’®ç‚¹å‡»
 const handleRecordClick = useCallback(async () => {
   // å¦‚æœå½“å‰æ­£åœ¨æ’­æ”¾æˆ–å¤„ç†ä¸­ï¼Œåˆ™å–æ¶ˆå½“å‰ä»»åŠ¡
   if (isAudioPlaying || processingRef.current) {
     cancelProcessingTask();
     return;
   }
   
   if (isRecording) {
     // åœæ­¢å½•éŸ³
     await stopRecording();
   } else {
     // å¼€å§‹å½•éŸ³
     if (micStream) {
       startRecording(micStream, setIsRecording, setRecordingStatus);
     } else {
       // å°è¯•é‡æ–°è·å–éº¦å…‹é£æƒé™
       const stream = await setupMicrophone(setMicStatus);
       if (stream) {
         setMicStream(stream);
         startRecording(stream, setIsRecording, setRecordingStatus);
       } else {
         alert("æ— æ³•è®¿é—®éº¦å…‹é£ã€‚è¯·æ£€æŸ¥æƒé™è®¾ç½®ã€‚");
       }
     }
   }
 }, [isRecording, micStream, stopRecording, isAudioPlaying, cancelProcessingTask]);

 // è·å–æœ€åä¸€æ¡æ¶ˆæ¯ç”¨äºæ˜¾ç¤º
 const lastMessage = conversation.length > 0 ? conversation[conversation.length - 1] : null;

 // è®¡ç®—éº¦å…‹é£æŒ‰é’®æ–‡æœ¬
 const getMicButtonText = useCallback(() => {
   if (isRecording) return 'â– ';
   if (isAudioPlaying || processingRef.current) return 'âœ•';
   return 'ğŸ¤';
 }, [isRecording, isAudioPlaying]);

 return (
   <div style={STYLES.container}>
     {/* å¯¹è¯å†å² (æ˜¾ç¤ºæœ€åä¸€æ¡) */}
     {lastMessage && (
       <div style={STYLES.conversationBox}>
         <div
           style={lastMessage.role === 'user' ? STYLES.userBubble : STYLES.aiBubble}
         >
           {lastMessage.content}
         </div>
       </div>
     )}

     {/* è¯­éŸ³è¾“å…¥åŒºåŸŸ */}
     <div style={STYLES.speechArea}>
       <button
         onClick={handleRecordClick}
         style={{
           ...STYLES.recordButton,
           ...(isRecording ? STYLES.recordingButton : {})
         }}
       >
         <span style={STYLES.micIcon}>
           {getMicButtonText()}
         </span>
       </button>

       <div style={STYLES.statusText}>
         {isRecording ? 'æ­£åœ¨å½•éŸ³...' : (statusMessage || 'ç‚¹å‡»éº¦å…‹é£å¼€å§‹è¯­éŸ³è¾“å…¥')}
       </div>
     </div>

     {/* çŠ¶æ€æ  */}
     <div style={STYLES.statusBar}>
       {backendStatus} | {wsReady ? 'WebSocketå·²è¿æ¥' : 'HTTPæ¨¡å¼'} | åŠ¨ç”»: {animReady ? 'å·²åŠ è½½' : 'æ— '} | {micStatus}
     </div>

     <Canvas dpr={2} onCreated={(ctx) => {
         ctx.gl.physicallyCorrectLights = true;
       }}>

       <OrthographicCamera
         makeDefault
         zoom={2000}
         position={[0, 1.65, 1]}
       />

       <Suspense fallback={null}>
         <Environment background={false} files="/images/photo_studio_loft_hall_1k.hdr" />
       </Suspense>

       <Suspense fallback={null}>
         <Bg />
       </Suspense>

       <Suspense fallback={null}>
         <Avatar
           avatar_url="/model.glb"
           speak={speak}
           setSpeak={setSpeak}
           text={response}
           playing={playing}
           setPlaying={setPlaying}
           setResponse={setResponse}
           setAnimReady={setAnimReady}
           animationData={animationData}
           isAudioPlaying={isAudioPlaying}
           currentSegmentIndex={currentAudioIndex}
         />
       </Suspense>

     </Canvas>
     <Loader dataInterpolation={(p) => `åŠ è½½ä¸­... ${Math.round(p * 100)}%`} />
   </div>
 )
}

function Bg() {
 const texture = useTexture('/images/bg.webp');

 return(
   <mesh position={[0, 1.5, -2]} scale={[0.8, 0.8, 0.8]}>
     <planeBufferGeometry />
     <meshBasicMaterial map={texture} />
   </mesh>
 )
}

export default App;
